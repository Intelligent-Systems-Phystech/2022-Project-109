\documentclass{article}
\usepackage{arxiv}

\usepackage[]{cite}
\usepackage{cmap}
\usepackage{amsmath, amsfonts,amssymb}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\newcommand\argmin{\mathop{\arg\min}}
\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
\newcommand{\hchi}{\hat{\boldsymbol{\chi}}}
\newcommand{\hphi}{\hat{\boldsymbol{\varphi}}}
\newcommand{\bchi}{\boldsymbol{\chi}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\hx}{\hat{x}}
\newcommand{\hX}{\hat{\X}}
\newcommand{\hy}{\hat{y}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\p}{p(\cdot)}
\newcommand{\cc}{\mathbf{c}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\e}{\mathbf{e}}
\newcommand{\h}{\mathbf{h}}
\newcommand{\q}{q(\cdot)}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\dd}{\partial}

\renewcommand{\baselinestretch}{1}
\renewcommand{\abstractname}{Аннотация}



\title{Непрерывное время при построении нейроинтерфейса}

\author{ Змушко Филипп \\
	МФТИ \\
	\texttt{zmushko.fa@phystech.edu} \\
	%% examples of more authors
	\And
	Самохина Алина \\
	МФТИ\\
	\texttt{alina.samokhina@phystech.edu} \\
	%% examples of more authors
	\And
	Стрижов Вадим \\
	МФТИ\\
	\texttt{strijov@phystech.edu} \\
}
\date{}

\renewcommand{\shorttitle}{Непрерывное время при построении нейроинтерфейса}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
% \hypersetup{
% pdftitle={A template for the arxiv style},
% pdfsubject={q-bio.NC, q-bio.QM},
% pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
% pdfkeywords={First keyword, Second keyword, More},
% }

\begin{document}
\maketitle

\begin{abstract}
	В задачах декодирования сигнала непрерывный процесс исследуется с помощью нейронных сетей, использующих дискретное представление о времени. Недавно появившиеся модели, основывающиеся на нейронных обыкновенных дифференциальных уравнений рассматривают временные ряды как непрерывные во времени.
    В данной работе исследуется задача декодирования сигнала через представление в виде непрерывной во времени функции. Проводится сравнение базовых моделей, использующих дискретное время, с моделями на основе нейронных дифференциальных уравнений: обыкновенных и управляемых. Исследуется применение моделей на основе нейронных дифференциальных уравнений как для регулярных сигналов, так и для сигналов с пропущенными значениями. 
\end{abstract}

\keywords{Нейронные ОДУ \and управляемые ДУ\and LSTM \and Irregular Time Series}

\section{Введение}
Данная статья посвящена  использованию непрерывного по времени сигнала при построении интерфейса мозг-компьютер (ИМК). ИМК~--- технология, позволяющая человеку взаимодействовать с компьютером посредством обработки данных об электрической активности мозга. Рассматривается задача декодирования сигнала электроэнцефалограмм (ЭЭГ). Основной сложностью при работе с сигналами ЭЭГ являются нерегулярные по времени данные. Нерегулярность возникает из-за зашумленности, обусловленной малым количеством датчиков и низкой частотой дискретизации. Для улучшения качества декодирования сигнала предлагается использовать модели на основе непрерывного по времени представления сигнала.

Один из подходов для получения непрерывного представления сигнала основывается на идее нейронных дифференциальных уравнений. Данная концепция, изложенная в \cite{NEURIPS2018_69386f6b}, рассматривает скрытое состояние ResNet как непрерывное во времени. Дальнейшее развитие этой идеи привело к появлению целого класса моделей, использующих непрерывное представление времени в рекуррентных нейронных сетях. В \cite{lechner2020longterm} рассматривается непрерывный аналог LSTM. Применение нейронных обыкновенных дифференциальных уравнений (NODE) к временным рядам с нерегулярным шагом по временной сетке представлено в \cite{cde}. В \cite{NEURIPS2019_952285b9} авторы рассматривают непрерывное представление во времени в виде разложения на полиномы Лежандра. 

В данной работе вышеперечисленные модели сравниваются на задаче декодирования сигнала. Изучена примененимость данных моделей для обработки нерегулярных во времени сигналов, получения непрерывного представления сигнала и построения нейроинтерфейса, работающего с непрерывным представлением о времени.

Модели на основе NODE, а также базовые модели (RNN, EEGNet\cite{Lawhern2018EEGNetAC}, ERPCov TS LR\cite{6046114}), использующие дискретное представление времени, сравниваются в рамках задачи декодирования сигнала на наборе данных потенциалов P300. Рассматриваемые здесь потенциалы P300~--- вызванные потенциалы, являющиеся специфическим откликом электрической активности мозга в ответ на внешний стимул.

Вышеперечисленные модели работают с непрерывным представлением временных рядов, но при этом не используют свойство непрерывности сигнала. В данной работе исследуется получение непрерывного по времени представления сигнала, которое в дальнейшем предлагается использовать при построении новых признаковых простанств. Полученные предлагается рассматривать как скрытые пространства, которые используются для согласовывания исходного сигнала с прогнозом с помощью корреляционного анализа.

\section{Постановка задачи классификации сигнала}


    
    Рассматривается задача классификации на выборке из регулярных по времени данных. Выборка делится на две части: обучающую и валидационную в соотношении 80:20.
    
    Выборка содержит $M$ (возможно, нерегулярных во времени) наблюдений. Тогда сигнал и целевая переменная определяются следующим образом:
    

    $$\X = \{\X_i\}_{i=1}^{M},$$
    $$\X_i = \{\x_t\}_{t\in T}, \ \x_t \in \R^E, \ T = \{t_i\}_{i=1}^{N},\ t_i \in \R$$ 
    
    $$\Y = \{y_i\}_{i=1}^{M},\ y_i \in \{0, 1\}$$
    
    $$E = 8 \text{~--- количество электродов},$$
    $$N = 40 \text{~--- количество наблюдений в одном отрезке ЭЭГ}.$$

    \paragraph{Постановка задачи классификации.}
    
    Решается задача бинарной классификации отрезков ЭЭГ. Требуется определить наличие в отрезке ЭЭГ потенциала P300.
    
    Для рассмотренных данных требуется получить целевую функцию:
    $$g_{\theta}: \X \to \Y.$$
    
    Критерием качества в данной задаче является бинарная кросс-энтропия: 
    $$L =  -{\frac {1}{M}}\sum _{m=1}^{M}\ {\bigg [}y_{m}\log {p}_{m}+(1-y_{m})\log(1-{p}_{m}){\bigg ]},$$
    $$p_m = g_{\theta}(\X_m) \text{ ~--- вероятность 1 класса для } \X_m.$$

    Решается оптимизационная задача:
    \begin{equation*}
    \hat{\theta} = \arg\max_{\theta} L(\theta, \X).
    \end{equation*}
    
    Внешними критериями качества для задачи классификации являются точность и F1-score.
    
\subsection{Построение непрерывного представления сигнала}\\
    
    Еще одной задачей данной работы является получение непрерывного представления сигнала. 
    Для наблюдаемого непрерывного процесса (активность мозга, движение):
    $$V(t), t \in \R.$$
    Задана выборка~--- реализация процесса $V(t)$:
    $$\X = \{\x_t\}_{t\in T},\  \x_t \in \R^E,\  T = \{t_i\}_{i=1}^{N}, t_i \in \R,$$ 
    $$\x_{t_i} \approx V(t_i).$$
    Предполагается, что можно получить:
    $$f_{\X}(t): \R \to \R^E, \ f_{\X}(t) = \x_t \approx V(t).$$

    В данной работе построение непрерывного представления рассматривается в рамках решения задачи классификации сигналов. Непрерывное представление извлекается из скрытых состояний рассматриваемых далее алгоритмов классификации.
    

\section{Используемые модели}

Для решения задачи классификации ЭЭГ используются следующие модели на основе непрерывного по времени представления сигнала: Neural CDE\cite{cde}, ODE-RNN\cite{NEURIPS2019_42a6845a}(?), ODE LSTM\cite{lechner2020longterm}, LMU LSTM\cite{NEURIPS2019_952285b9}, ODE-GRU-D\cite{Habiba2020NeuralOF}(?). В этом разделе мы подробнее рассмотрим архитектуру этих моделей.

\subsection{Neural CDE}

Данная модель, представленная в \cite{cde}, позволяет работать с нерегулярными временными рядами.

Пусть $\mathbf{X} = (\mathbf{x}_0, \mathbf{x}_1 ..., \mathbf{x}_n), x_i \in \mathbb{R}^E$ ~--- дискретные наблюдения некоторого непрерывного процесса в моменты времени $t_i\in \mathbb{R}$ такие, что $t_0 < \dots < t_n$. 

Скрытое состояние Neural CDE определяется как решение управляемого дифференциального уравнения 

$$
\mathbf{z}_{t}=\mathbf{z}_{t_{0}}+\int_{t_{0}}^{t} f_{\theta}\left(\mathbf{z}_{\tau}\right) \mathrm{d} S_{\tau}, \quad t \in\left(t_{0}, t_{n}\right],
$$

где $S_\mathbb{X}(t) : \left[t_0, t_n \right] \rightarrow \mathbb{R}^{E+1}$ ~--- натуральный кубический сплайн с узлами в $t_0, \dots, t_n$ ~--- аппроксимация исходного процесса, $f_{\theta}: \mathbb{R}^{d} \rightarrow \mathbb{R}^{d\times(E+1)}$ ~--- нейронная сеть с параметрами $\theta$, где $d$ ~--- размерность скрытого пространства. Интеграл в правой части равенства понимается как интеграл Римана-Стилтьеса.

Начальное скрытое состояние $\mathbf{z}_{t_0} = i_{\theta}(\mathbf{x}_0, t_0)$ определяется нейронной сетью $i_{\theta}: \R^{E+1} \rightarrow \R^{d}$, а значение скрытого состояния в момент $\tau \in\left(t_0, t_n\right]$ получается в результате решения указанного выше управляемого дифференциального уравнения с помощью ODESolve().

Предсказания в рамках классификации получаются как результат применения к $\mathbf{z}_{t_n} $функции $r_{\theta} : \R^{d} \rightarrow \{0, \dots, c\}$, где $c$ ~--- количество классов. 

\subsection{ODE-RNN}

Данная модель\cite{NEURIPS2019_42a6845a} является непрерывным аналогом RNN. 

Скрытое состояние RNN изменяется в ячейке RNNCell, которая задается формулой:

$$
\mathbf{h}_i = \tanh(\mathbf{W}_{hh} \mathbf{h}_{i-1} + \mathbf{b}_{hh} + \mathbf{W}_{ih}\mathbf{x}_i + \mathbf{b}_{ih})
$$

В случае непрерывного аналога следующее значение скрытого слоя $\mathbf{h}_i$ получается в 2 шага. Сначала с помощью ODESolve находится промежуточное значение 

$$\mathbf{h}_i^{\prime} = \text{ODESolve}(f_{\theta}, \mathbf{h}_{i-1}, (t_{i-1}, t_i))$$,

где $f_{\theta}$ ~--- обучаемая функция с параметрами $\theta$. Затем $\mathbf{h}_i^{\prime}$ подается на вход в стандартную ячейку RNNCell:

$$\mathbf{h}_i = \text{RNNCell}(\mathbf{h}_i^{\prime}, \mathbf{x}_i)$$

Предсказания классификации получаются в результате преобразования последнего значения скрытого слоя $\mathbf{h}_n$. 

\subsection{ODE LSTM}

Как показано в \cite{lechner2020longterm}, модель ODE LSTM решает проблему взрыва и затухания градиента, которой подвержены модели CDE и ODE-RNN.

Скрытым состоянием стандартной модели LSTM является пара $(\mathbf{c}_t, \mathbf{h}_t)$, где $\mathbf{h}_t$ ~--- обычное скрытое состояние, а $\mathbf{c}_t$ ~--- состояние долгосрочной памяти. Изменение скрытого состояние происходит в ячейке LSTMCell по следующим формулам:

\begin{equation*}
\begin{array}{l}

\mathbf{z}_{t}=\tanh \left(\W_{z} \x_{t}+\mathbf{R}_{z} \h_{t-1}+\bb_{z}\right)\\
\mathbf{i}_{t}=\sigma\left(\W_{i} \x_{t}+\mathbf{R}_{i} \h_{t-1}+\bb_{i}\right)\\
\mathbf{f}_{t}=\sigma\left(\W_{f} \x_{t}+\mathbf{R}_{f} \h_{t-1}+\bb_{f}+\mathbf{1}\right)\\
\mathbf{o}_{t}=\sigma\left(\W_{o} \x_{t}+\mathbf{R}_{o} \h_{t-1}+\bb_{o}\right)\\
\cc_{t}=\mathbf{z}_{t} \odot \mathbf{i}_{t}+\cc_{t-1} \odot \mathbf{f}_{t} \\
\h_{t}=\tanh \left(\cc_{t}\right) \odot \mathbf{o}_{t}
\end{array}
\end{equation*}

Весами в данной модели являются $\mathbf{W}_x, \mathbf{R}_x$ и $\mathbf{b}_x, x\in \{z, i, f, o\}$. 

В случае непрерывного аналога добавляется дополнительная обработка выходного значения скрытого состояния. Сначала находится промежуточное значение $\h_t^{\prime}$:

$$(\cc_t, \h_t^{\prime}) = \text{LSTMCell}\left(\theta_{l},\left(\cc_{t-1}, \mathbf{h}_{t-1}\right), \x_{t}\right)$$,

а затем к нему применяется ODESolve:

$$\mathbf{h}_{t}=\text { ODESolve}\left(f_{\theta}, \mathbf{h}_{t-1}, \mathbf{h}_{t}^{\prime}, t_{t}-t_{t-1}\right)$$

Как и для предыдущих моделей, предсказания в рамках задачи классификации получаются в результате применения функции классификации к выходу последнего слоя.

\subsection{LMU LSTM}

Основной компонент LMU ~--- ячейка памяти, ортогонализующая непрерывную по времени историю изменения сигнала  $u(t) \in \mathbb{R}$, по скользящему окну $\theta \in \mathbb{R}_{>0}$. Ячейка построена по аналогии с линейной функцией переноса для непрерывной задержки, $F(s)=e^{-\theta s}$, которую наилучшим образом аппроксимируют $d$ связанных ОДУ:
$$
\theta \dot{\mathbf{c}}(t)=\mathbf{A} \mathbf{c}(t)+\mathbf{b} u(t),
$$
где $\mathbf{c}(t) \in \mathbb{R}^{d}$ вектор состояния долгосрочной памяти размерности $d$. Наилучшие матрицы $(\mathbf{A}, \mathbf{b})$, получаются при использовании аппроксимантов Паде \cite{voelkerthesis}:

$$
\begin{array}{c}
\mathbf{A}=[a]_{i j} \in \mathbb{R}^{d \times d}, \quad a_{i j}=(2 i+1)\left\{\begin{array}{ll}
-1 & i<j \\
(-1)^{i-j+1} & i \geq j
\end{array}\right. \\
\mathbf{b}=[b]_{i} \in \mathbb{R}^{d \times 1}, \quad b_{i}=(2 i+1)(-1)^{i}, \quad i, j \in[0, d-1]
\end{array}
$$
Основное свойство данной динамической системы заключается в том, что $\mathbf{c}$ представляет собой скользящее окно $u$ по полиномам Лежандра степени до $d-1$ :
$$
u\left(t-\theta^{\prime}\right) \approx \sum_{i=0}^{d-1} \mathcal{P}_{i}\left(\frac{\theta^{\prime}}{\theta}\right) c_{i}(t), \quad 0 \leq \theta^{\prime} \leq \theta, \quad \mathcal{P}_{i}(r)=(-1)^{i} \sum_{j=0}^{i}\left(\begin{array}{c}
i \\
j
\end{array}\right)\left(\begin{array}{c}
i+j \\
j
\end{array}\right)(-r)^{j},
$$
где  $\mathcal{P}_{i}(r)$  ~--- сдвинутые полиномы Лежандра. Таким образом может быть получена единственная и оптимальная декомпозиция, в которой $\mathbf{c}$ отвечает вычислениям в окнах размера $\theta$, спроецированным на $d$ ортогональных базисных функций.

$$
\mathbf{h}_{t}=\tanh\left(\mathbf{W}_{\mathbf{x}} \mathbf{x}_{t}+\mathbf{W}_{\mathbf{h}} \mathbf{h}_{t-1}+\mathbf{W}_{\mathbf{c}} \mathbf{c}_{t}\right)
$$

$$
u_{t}=\mathbf{e}_{\mathbf{x}}^{\top} \mathbf{x}_{t}+\mathbf{e}_{\mathbf{h}}{ }^{\top} \mathbf{h}_{t-1}+\mathbf{e}_{\mathbf{c}}^{\top} \mathbf{c}_{t-1}
$$

В данных уравнениях $\mathbf{W}_{\mathbf{x}}, \mathbf{W}_{\mathbf{h}}, \mathbf{W}_{\mathbf{c}}$ ~--- матрицы весов линейных слоёв LSTM,  $\mathbf{e}, \mathbf{e}_{\mathbf{h}}, \mathbf{e}_{\mathbf{m}}$ ~--- векторы, кодирующие соответствующие входы LSTM.



\section{Вычислительный эксперимент}

\subsection{Цель эксперимента}

Сравнить результаты работы алгоритмов на основе NODE и алгоритмов, использующих дискретное представление времени. Провести данное сравнение как в случае регулярных сигналов, так и в случае данных с частично пропущенными значениями. Для моделей на основе NODE получить непрерывное во времени представление скрытого состояния.

\subsection{Экспериментальные данные}

\subsubsection{Набор данных P300}

Набор данных записывался с помощью энцефалографа NVX-52 (МКС, Зеленоград) с частотой 500 Гц. Для записи использовались 8 губчатых электродов (Cz, P3, P4, PO3, POz, PO4, O1, O2). Стимулы предъявлялись
с помощью шлема HTC Vive Pro VR.

Эксперимент был основан на игре в виртуальной реальности с нейроуправлением, основанным на класификации потенциала P300. Процесс игры заключался в кормлении енотов и защите их от демонов. В ходе обучения участникам указывался один из пяти енотов, на котором нужно было сконцентрироваться. Каждый акт игры состоял из случайных активаций различных енотов. В одном акте происходило по 10 активаций каждой цели, и для одного участника обучение состояло из 5 актов. Таким образом на выходе этапа обучения получалось 500 отрезков ЭЭГ для обучения классификатора: 50 целевых отрезков, содержащих потенциал P300 и 450 нецелевых.

На этапе обратной связи стимулами служили демоны. Участник должен был концентрироваться на выбранном демоне для уничтожения цели, которое происходило в конце акта.

Суммарное количество отрезков ЭЭГ в полученном наборе данных: 56540.

\subsubsection{Другой/ие датасет/ы?}

\subsection{Структура эксперимента}

Набор данных делился на обучающую и тестовую выборки в соотношении 80:20. Для эксперимента на данных с частично пропущенными значениями предварительно удалялись 20\% случайных точек. Для обучения использовался алгоритм оптимизации Adam. В качестве критерия оптимизации была взята кросс-энтропия. Сравнение результатов проводилось с помощью метрик точности и F1-score. 

\subsection{Результаты эксперимента}

(Появятся потом)


\section{Список графиков}

Зависимость качества (метрик) от количества параметров (размерности скрытых пространств) для всех моделей

Зависимость качества от количества пропущенных данных

Зависимость качества от количества эпох (?)





\bibliographystyle{unsrt}


\bibliography{Zmushko2022ContinuousTime}



\end{document}